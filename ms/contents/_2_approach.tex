\section{Materials and Methods}

\subsection*{Cow Husbandry}

The studied cows were housed in a free-stall barn at Virginia Tech Dairy Comlex at Kentland Farm in Virginia, USA. The cow handling and image capturing were conducted following the guidelines and approval of the Virginia Tech Institutional Animal Care and Use Committee (\#IACUC xxxxx).

\subsection*{Image Dataset}

The images in this study were collected using the Amazon Ring camera model Spotlight Cam Battery Pro (Ring Inc.), which offers a real-time video feed of dairy cows. Three cameras were installed in the barn: two at a height of 3.25 meters (10.66 feet) above ground covering an area of 33.04 square meters (355.67 square feet). One camera provided a top view while the other was angled approximately 40 degrees from the horizontal to offer a side view of the cows. These are hereafter referred to as \textit{the top-view camera} and \textit{the side-view camera}, respectively. A third camera, termed \textit{the external camera}, was set at a lower height of 2.74 meters (9.00 feet) and covered a larger area of 77.63 square meters (835.56 square feet). Positioned 10 degrees downward from the horizontal, it captured a challenging perspective prone to occlusions among cows.

Images were captured using an unofficial Ring Application Programming Interface (API) \citep{greif_dgreifring_2024}, configured to record a ten-second video clip every 30 minutes continuously for 14 days. Since the image quality relies on the camera's internet connection, which was occasionally unstable, some images were found to be tearing or unrecognizable. Hence, the resulting dataset was manually curated for consistent quality, comprising 504 images from \textit{the top-view camera}, 500 from \textit{the side-view camera}, and 250 from \textit{the external camera}. These images were futher categorized based on the lighting conditions: for \textit{the top-view camera}, 296 images were captured during daylight, 118 in the evening under artificial lighting, and 90 as near-infrared images without artificial light. From \textit{the side-view camera}, 113 images were taken in the evening, and 97 as near-infrared images. All images from \textit{the external camera} were captured during the day. The image examples were shown xxx.

The image annotations were conducted using an online platform, Roboflow \citep{}, to define cow positions in the images. The bounding boxes were manually drawn to enclose the cow contours, providing the coordinates of the top-left corners and the width and height of the boxes. If cows were partially occluded, the invisible parts were infered based on the adjacent visible parts. If the cow position was too far from the camera and make the important body features, such as head, tail, and legs, unrecognizable, the cow was excluded from the dataset. The final annotations were saved in the YOLO format \citep{}, where annotations were stored in a text file with one row per cow in the image, each row containing the cow's class, center coordinates, width, and height of the bounding box. The graphical representation of the annotated images was shown in Figure XXX.

\subsection*{Study 1: Benchmarking Model Generalization Across Different Environmental Conditions}

To compare the performance drop between different view angles and lighting conditions, we designed a cross-testing strategy where models were trained on one dataset configuration and tested on another. There are five training configurations in this study:

\begin{itemize}
    \item \textbf{Baseline:} The model was trained and evaluated on the dataset characterized for all the conditions including top-view, side-view, daylight, evening, and near-infrared images. The images were not overlapped between the training and evaluation sets.
    \item \textbf{Top2Side:} The model was trained on the top-view images and evaluated on the side-view images.
    \item \textbf{Side2Top:} The model was trained on the side-view images and evaluated on the top-view images.
    \item \textbf{Day2Night:} The model was trained on the daylight images and evaluated on the evening images, including both artificial lighting and near-infrared images.
    \item \textbf{External:} The model was trained on images collected by the top-view and side-view cameras and evaluated on the external camera images.
\end{itemize}

To study how the training sample size affects the model performance in each configuration, the testing set in the cross-validation was first fixed to the same 100 images. Then, the training set size was altered iteratively from 16 to 512 images with a step size of doubling the sample size. Each training sample size was repeated 50 times with different random seeds to ensure the robustness of the results. The YOLOv9e, which is the most capable model in the YOLO family to date according to the performance on the COCO datset, was used as the base model for this study.


\subsection*{Study 2: The correlation between model complexity and performance on the tasks of localizing cows}

To investigate whether the model performance increases with the model complexity, five YOLO-family models were investigated in thie study. Three modesl were selected from the YOLOv8 family: YOLOv8n, YOLOv8m, and YOLOv8x. They are different in the number of layers and the width of the network, resulting in model parameters of 3.2 millions (m), 25.9m, and 68.2m, respectively. The other two models were YOLOv9c and YOLOv9e, which are the latest models in the YOLO family. They are different in model configuration and have 25.3m and 57.3m parameters, respectively. All models were trained on 500 images in each of four configurations described in Study 1.

In addition to the model performance, the computing speed was also evaluated. The training speed was recorded in miuntes per epoch, and the inference time was recorded as frames per second (FPS) on both the CPU and GPU (Apple M1 Max chip, Apple Inc.) The relationship between the model complexity and the time consumption was analyzed to provide insights into the trade-off between the model performance and the computational cost.

\subsection*{Study 3: Assessing the advantages of using fine-tuned model over the pre-trained model as initial model weights}

Having a model trained on a smaller but specific dataset is expected to have a better performance in similar specific tasks than a model pre-trained on a larger but general dataset (e.g., COCO dataset). However, such advantages may not necessarily persist as the training sample size increases. Having equally enough samples for both the pre-trained and fine-tuned models could diminish the performance gap between the two models. To investigate this hypothesis,







\subsection*{Model Training}



\subsection*{Model Evaluation}

The examined YOLO models are object detection models that return positions of detected objects (i.e., cows in this study) for the evaluated images. The detections are represented by a list of boudning boxes. Regardless of specific procedures among YOLO variants for computational efficiency, such as YOLOv8 that integrate objectness scores and conditional class probabilities into a single confidence score, each detection generally consist of $4+c$ elements: the xy-coordinates, width, and height of the bounding box, and the $c$ confidence scores indicating the probability of the object belonging to each of the $c$ classes. The class with the highest confidence score is considered the predicted class of the object. To evaluate the model performance, two aspects are considered: the localization accuracy and the classification accuracy. The localization accuracy is measured by the Intersection over Union (IoU) between the predicted bounding box and the ground truth bounding box. On the other hand, the classification accuracy is measured by the precision and recall given the confidence threshold. If the confidence score of a detection is higher than the threshold, the detection is considered as a positive detection. Otherwise, the detection is neglected. Combining the localization and classification accuracy, the mean Average Precision ($mAP$) averaged the area under the precision-recall curve across all the classes. The curve is generated by varying the confidence threshold from 0 to 1 given a IoU threshold. In this study, four metrics were used in the evaluation: the precision and recall at the confidence threshold of 0.25 and IoU threshold of 0.5, the mAP at the IoU threshold of 0.5 (noted as $mAP_{0.5}$), and the averaged mAP at varying IoU thresholds ranging from 0.5 to 0.95 (noted as $mAP_{0.5-0.95}$).


