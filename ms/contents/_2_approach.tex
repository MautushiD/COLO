\section{Materials and Methods}

\subsection*{Cow Husbandry}

The studied cows were housed in a free-stall barn at Virginia Tech Dairy Comlex at Kentland Farm in Virginia, USA. The cow handling and image capturing were conducted following the guidelines and approval of the Virginia Tech Institutional Animal Care and Use Committee (\#IACUC xxxxx).


\subsection*{Image Dataset}

The images in this study were collected using the Amazon Ring camera model Spotlight Cam Battery Pro (Ring Inc.), which offers a real-time video feed of dairy cows. Three cameras were installed in the barn: two at a height of 3.25 meters (10.66 feet) above ground covering an area of 33.04 square meters (355.67 square feet). One camera provided a top view while the other was angled approximately 40 degrees from the horizontal to offer a side view of the cows. These are hereafter referred to as \textit{the top-view camera} and \textit{the side-view camera}, respectively. A third camera, termed \textit{the external camera}, was set at a lower height of 2.74 meters (9.00 feet) and covered a larger area of 77.63 square meters (835.56 square feet). Positioned 10 degrees downward from the horizontal, it captured a challenging perspective prone to occlusions among cows.

Images were captured using an unofficial Ring Application Programming Interface (API) \citep{greif_dgreifring_2024}, configured to record a ten-second video clip every 30 minutes continuously for 14 days. Since the image quality relies on the camera's internet connection, which was occasionally unstable, some images were found to be tearing or unrecognizable. Hence, the resulting dataset was manually curated for consistent quality, comprising 504 images from \textit{the top-view camera}, 500 from \textit{the side-view camera}, and 250 from \textit{the external camera}. These images were futher categorized based on the lighting conditions: for \textit{the top-view camera}, 296 images were captured during daylight, 118 in the evening under artificial lighting, and 90 as near-infrared images without artificial light. From \textit{the side-view camera}, 113 images were taken in the evening, and 97 as near-infrared images. All images from \textit{the external camera} were captured during the day. The image examples were shown xxx.

The image annotations were conducted using an online platform, Roboflow \citep{}, to define cow positions in the images. The bounding boxes were manually drawn to enclose the cow contours, providing the coordinates of the top-left corners and the width and height of the boxes. If cows were partially occluded, the invisible parts were infered based on the adjacent visible parts. If the cow position was too far from the camera and make the important body features, such as head, tail, and legs, unrecognizable, the cow was excluded from the dataset. The final annotations were saved in the YOLO format \citep{}, where annotations were stored in a text file with one row per cow in the image, each row containing the cow's class, center coordinates, width, and height of the bounding box. The graphical representation of the annotated images was shown in Figure XXX.

\subsection*{Data Split Design}

\subsubsection*{Study 1: Benchmarking Model Generalization Across Different Conditions}

To compare the performance drop between different view angles and lighting conditions, we designed a cross-testing strategy where models were trained on one dataset configuration and tested on another. There are four dataset configurations in this study: 






Our study starts with the systematic acquisition of image data, focusing on targeted cattle populations within Kentland Farm at Virginia Tech. All animal handling and media recordings were conducted following the guidelines and approval of the Virginia Tech Institutional Animal Care and Use Committee. This initial phase is succeeded by meticulous data processing steps which include the annotation and formatting of the dataset for machine learning applications. Subsequently, we proceed to fine-tune our dataset utilizing a variety of deep learning architectures. For each model, we meticulously calculate a suite of performance metrics.

Building upon the results obtained from these diverse models, we construct a "summary plot." This plot is designed to elucidate the findings related to the second and third questions delineated in Section {contribution} of our paper. It will visually guide the selection of an optimal model by delineating the relationship between dataset size and achieved accuracy, as well as the computational cost versus the precision of the models. Through this analytical representation, we aim to furnish a comprehensive tool that aids researchers in making informed decisions when it comes to choosing the most suitable object detection model for their specific requirements in livestock production studies.

\subsection*{Data Preparation}


Recognizing the crucial role of lighting conditions on data integrity, we meticulously orchestrated our data gathering operations at assorted intervals throughout the day, specifically: dawn, midday, dusk, and late evening. This methodical approach was paramount in guaranteeing the inclusion of an extensive spectrum of lighting conditions within our dataset, thereby augmenting its diversity and resilience to various environmental challenges.

Moreover, cognizant of the effect camera angles and perspectives have on capturing the full gamut of cattle postures, we varied our image capture process accordingly. This variation not only accounted for the different positions and movements of the cattle but also for the heterogeneous nature of the environment in which they were situated. In addition, we aimed to ensure a broad representation of breeds by including both Jersey and Holstein cows in our dataset, recognizing that breed-specific characteristics could significantly influence the model's performance.

\subsubsection*{Animal Husbandry}


The farm, hosting a diverse bovine community of over 200 individuals from the Jersey and Holstein breeds, served as an exemplary setting for our endeavor. It offered a plethora of varied scenarios and animal interactions, encapsulating the essence of a vibrant and dynamic agricultural environment. This multifaceted setting was critical in establishing a robust and comprehensive dataset reflective of the real-world complexity and variability one would expect in a livestock farming operation.



\subsubsection*{Data Annotation and Formation}

- Talk about how the data was collected using pipes and Amazon Ring Cameras

- How the data is annotated on Roboflow
The methodological rigor involved in data preparation is vital for the integrity of our machine learning model's training process. Here is a clear outline of the steps executed in preparing the dataset for cow detection in our investigation:

\begin{enumerate}
    \item \textbf{Frame Extraction:} We utilized a customized Python script for the extraction of frames from the video streams, converting them into a series of static images. To guarantee uniformity across the dataset, we extracted frames at regular one-second intervals. This process yielded `n' distinct images, which were then allocated to training, validation, and testing datasets for the subsequent stages of our machine learning endeavor.


    \item \textbf{Annotation with Roboflow:} We uploaded the frames onto Roboflow, a versatile annotation platform. This tool enabled our team to annotate images by meticulously outlining cows with bounding boxes, ensuring that the AI model can learn to identify the target objects effectively. The annotated frames are exemplified in Figure ~\ref{fig:camera-roboflow}.

    \item \textbf{Annotation Format Selection:} Roboflow's robust export options allowed us to obtain annotations in various formats suitable for different model architectures. We primarily opted for COCO and YOLOv5 formats, both widely recognized for their compatibility with state-of-the-art object detection algorithms.

    \item \textbf{Data Storage and Maintenance:} Post-annotation, we stored the images in the universally accepted JPG format. Accompanying these images, the corresponding annotation files were meticulously cataloged, readying the dataset for the intricate process of model training and subsequent evaluation.
\end{enumerate}
By adhering to these steps, we ensured the creation of a high-quality, standardized dataset poised for deployment in the development of an AI-powered cow detection system, geared towards enhancing the precision and efficiency of livestock management.

\subsection*{Simulation Design}

\subsubsection*{Data Splits}
We aim to thoroughly investigate model generalization across diverse conditions within livestock environments, specifically focusing on cattle localization. To achieve this, we meticulously designed and organized our dataset into five distinct configurations, each representing unique conditions under which the cattle were captured. These configurations are critical for evaluating the robustness and adaptability of object detection models, particularly in terms of their ability to generalize from one set of conditions to another. Below, we detail the dataset configurations and the rationale behind our data split strategy.

Dataset Configurations:
\begin{enumerate}
 \item\textbf{Top View:} Images captured from an overhead perspective, providing a comprehensive view of the livestock area.
 \item\textbf{Side View:} Images taken at a 60-degree (approx.) angle to the ground, offering a profile perspective of the cattle.
 \item\textbf{Daylight:} Images captured during daylight conditions from both the top and side views, ensuring natural lighting.
 \item\textbf{Nighttime:} Images obtained during nighttime from both the top and side views, with lighting conditions significantly reduced.
 \item\textbf{Breed Specific:} A subset of images exclusively featuring the Holstein breed, allowing for breed-specific model training.
 \end{enumerate}
Training and Testing Strategy
To rigorously assess model generalization, we employed a cross-testing methodology where models were trained on one dataset configuration and tested on another. This approach enabled us to isolate and understand the impact of various factors—such as viewing angle, lighting conditions, and breed variation—on model performance. The specific training and testing scenarios were as follows:
\begin{enumerate}
 \item\textbf{Viewing Angle Generalization:} Models were trained on the Top View dataset and tested on the Side View dataset, and vice versa. This setup assesses the model's ability to adapt to changes in perspective.
 \item\textbf{Lighting Condition Generalization:} Models trained on Daylight data were tested on Nighttime data to evaluate performance under varying lighting conditions, and vice versa.
\item\textbf{Breed Variation Generalization:} Models trained on the Breed Specific (Holstein) dataset were tested on a mixed-breed dataset (Holstein and Jersey), assessing the impact of breed diversity on detection accuracy.
 \item\textbf{Comprehensive Generalization:} Finally, models were trained on a combination of all dataset configurations to examine overall generalization capabilities across viewing angles, lighting conditions, and breed variations.
 \end{enumerate}
This structured approach to data split and testing is designed to provide insights into the extent to which object detection models, trained under specific conditions, can accurately generalize to different, untrained conditions. By systematically varying training and testing datasets, we aim to uncover potential limitations and strengths of current object detection technologies in the context of livestock monitoring, contributing valuable knowledge towards the development of more robust and adaptable solutions in precision agriculture.

\subsubsection*{Objective 1: How model performance is decomposed by different factors}
To investigate the model generalization. Factors such as lightning..

Each data configuration

\subsubsection*{Objective 2: How a fine-tuned model performed on a new dataset}

\subsection*{Model Training and Evaluation}

\subsubsection*{Trianing hyperparameters}

- how to cross validation is Design with different sample Size
- Iteration
- Evaluation metrics

\subsubsection*{Data Augmentation}

\subsubsection*{Model evaluation and cross validation}


% -------------------
